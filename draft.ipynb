{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jElLULrDhQZR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "g4Wzg69bnwK2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TensorFlow version: 2.2.0\nEager execution: True\n"
        }
      ],
      "source": [
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "mode = \"gat\"\n",
        "hidden_units = \"16,16\"\n",
        "heads = \"8,8,1\"\n",
        "epochs = 5\n",
        "lr = 0.1\n",
        "dropout = 0.2\n",
        "file_dir = '/Users/lsj/Downloads/DeepInf-homework/weibo'\n",
        "batch = 1024\n",
        "train_ratio = 75\n",
        "vaild_ratio = 12.5\n",
        "instance_normalization = True\n",
        "use_vertex_feature = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_w2v_feature(file, max_idx=0):\n",
        "    with open(file, \"rb\") as f:\n",
        "        nu = 0\n",
        "        for line in f:\n",
        "            content = line.strip().split()\n",
        "            nu += 1\n",
        "            if nu == 1:\n",
        "                n, d = int(content[0]), int(content[1])\n",
        "                feature = [[0.] * d for i in range(max(n, max_idx + 1))]\n",
        "                continue\n",
        "            index = int(content[0])\n",
        "            while len(feature) <= index:\n",
        "                feature.append([0.] * d)\n",
        "            for i, x in enumerate(content[1:]):\n",
        "                feature[index][i] = float(x)\n",
        "    for item in feature:\n",
        "        assert len(item) == d\n",
        "    return np.array(feature, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c26a5a090726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_vertex_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_w2v_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vertex_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ],
      "source": [
        "vertices = np.load(os.path.join(file_dir, \"vertex_id.npy\"))\n",
        "embedding_path = os.path.join(file_dir, \"deepwalk.emb_64\")\n",
        "max_vertex_idx = np.max(vertices)\n",
        "embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
        "print(embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graphs = np.load(os.path.join(file_dir, \"adjacency_matrix.npy\")).astype(np.float32)\n",
        "# self-loop trick, the input graphs should have no self-loop\n",
        "identity = np.identity(graphs.shape[1])\n",
        "graphs += identity\n",
        "graphs[graphs != 0] = 1.0\n",
        "if model == \"gat\":\n",
        "   graphs = graphs.astype(np.dtype('B'))\n",
        "elif model == \"gcn\":\n",
        "    # normalized graph laplacian for GCN: D^{-1/2}AD^{-1/2}\n",
        "    for i in range(len(graphs)):\n",
        "        graph = graphs[i]\n",
        "        d_root_inv = 1. / np.sqrt(np.sum(graph, axis=1))\n",
        "        graph = (graph.T * d_root_inv).T * d_root_inv\n",
        "        graphs[i] = graph\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "# wheather a user has been influenced\n",
        "# wheather he/she is the ego user\n",
        "influence_features = np.load(\n",
        "                os.path.join(file_dir, \"influence_feature.npy\")).astype(np.float32)\n",
        "\n",
        "labels = np.load(os.path.join(file_dir, \"label.npy\"))\n",
        "\n",
        "vertices = np.load(os.path.join(file_dir, \"vertex_id.npy\"))\n",
        "\n",
        "graphs, influence_features, labels,vertices = sklearn.utils.shuffle(\n",
        "                        graphs, influence_features,\n",
        "                        labels, vertices,\n",
        "                        random_state=47)\n",
        "\n",
        "vertex_features = np.load(os.path.join(file_dir, \"vertex_feature.npy\"))\n",
        "vertex_features = preprocessing.scale(vertex_features)\n",
        "\n",
        "embedding_path = os.path.join(file_dir, \"deepwalk.emb_64\")\n",
        "max_vertex_idx = np.max(vertices)\n",
        "embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GAT(keras.layers.Layer):\n",
        "    def __init__(self, n_head, f_in, f_out, attn_dropout):\n",
        "        super(GAT, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        w_init = tf.keras.initializers.GlorotUniform()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(n_head, f_in, f_out), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.a_src = tf.Variable(\n",
        "            initial_value=w_init(shape=(n_head, f_out, 1), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.a_dst = tf.Variable(\n",
        "            initial_value=w_init(shape=(n_head, f_out, 1), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(f_out,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "        self.leaky_relu = tf.nn.leaky_relu(alpha=0.2)\n",
        "        self.softmax = tf.nn.softmax(axis=-1)\n",
        "        self.dropout = tf.nn.dropout(attn_dropout)\n",
        "        self.permute = tf.keras.layers.Permute()\n",
        "    def call(self, inputs):\n",
        "        h, adj = inputs\n",
        "        bs, n = h.shape[:2]\n",
        "        h_prime = tf.matmul(tf.expand_dims(h,1),self.w)\n",
        "        attn_src = tf.matmul(tf.math.tanh(h_prime),self.a_src)\n",
        "        attn_dst = tf.matmul(tf.math.tanh(h_prime),self.a_dst)\n",
        "        s1 = attn_src.shape\n",
        "        s2 = attn_dst.shape\n",
        "        attn_src_b = tf.broadcast_to(attn_src,s1[0],s1[1],s1[2],n)\n",
        "        attn_dst_b = tf.broadcast_to(attn_dst,s2[0],s2[1],s2[2],n)\n",
        "        attn = attn_src_b + self.permute((1,2,4,3))(attn_dst_b)\n",
        "\n",
        "        attn = self.leaky_relu(attn)\n",
        "       ## mask = 1 - tf.expand_dims(adj,1)\n",
        "        ## todo: mask fill\n",
        "        \n",
        "        attn  = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        return tf.matmul(attn, h_prim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vertices_inputs = keras.Input(shape=(None,))\n",
        "feature_inputs = keras.Input(shape=(None,))\n",
        "graph_inputs = keras.Input(shape=(None,))\n",
        "\n",
        "##todo: emb init\n",
        "#emb = keras.layers.Embedding(embedding.shape[0],embedding.shape[1],embeddings_initializer=xxxx)\n",
        "##todo: instance normalization\n",
        "##emb = tfa.layers.InstanceNormalization(emb)\n",
        "#x = keras.layers.Concatenate(feature_inputs,emb)\n",
        "x = feature_inputs\n",
        "#x = keras.layers.Embedding(vertex_features.shape(0),ertex_features.shape(1), embeddings_initializer=xxx)\n",
        "\n",
        "##todo: multi head\n",
        "x = keras.layers.AdditiveAttention(x,graph_inputs) \n",
        "outputs = keras.layers.Softmax(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        #print(data)\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        trainable_vars = filter(lambda x: 'embedding' not in x.name ,trainable_vars)\n",
        "        for v in trainable_vars:\n",
        "           print(v.name)\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CustomModel(\n",
        "    inputs=[vertices_inputs, feature_inputs, graph_inputs],\n",
        "    outputs=[outputs],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QOsi6b-1CXIn"
      },
      "outputs": [],
      "source": [
        "## todo: tf nllloss?\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adagrad(lr=lr),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=?\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    {\"vertices\": vertices , \"feature\":influence_features , \"graph\": graphs},\n",
        "    {\"output\": outputs},\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "custom_training_walkthrough.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "name": "python_defaultSpec_1595269995667"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
